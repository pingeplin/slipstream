# TDD Implementation Plan: Pipeline Parallelization

This plan outlines the steps to parallelize the Slipstream CLI pipeline (Download -> OCR -> LLM) as
identified in [cli_workflow_slow_issues.md](../docs/cli_workflow_slow_issues.md).

## 1. Analysis of Existing Tests

### Tests to Keep (Functional Baselines)

- `tests/integration/test_cli_workflow.py`: These are critical end-to-end tests. They should remain
  mostly unchanged, but assertions might need to be more flexible regarding the order of output
  messages.
- `tests/unit/test_url_parser.py`: Unaffected.
- `tests/unit/test_models.py`: Unaffected.

### Tests to Update

- `tests/unit/test_cli.py`: Mocks for `GDriveClient.download_files` will need to change if the
  internal implementation changes from a bulk download to individual async downloads.
- `tests/unit/test_gdrive_parallel.py`: This test might need refactoring if `GDriveClient`'s
  parallelization logic is moved or changed.

### Tests to Remove

- None.

## 2. New Test Cases (Red Phase)

| Test Case                            | Description                                                                        | Expected Behavior                                                                               |
|--------------------------------------|------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|
| `test_pipeline_is_actually_parallel` | Mock Download, OCR, and LLM with `time.sleep` or `asyncio.sleep`. Process 3 files. | Total time should be slightly more than 1x duration, not 3x.                                    |
| `test_pipeline_error_isolation`      | Force one file to fail during OCR while others succeed.                            | One error message is reported, but other files complete processing and their results are shown. |
| `test_async_api_concurrency`         | Mock `AnthropicExtractor` and count concurrent calls.                              | Multiple calls to Anthropic should be in flight simultaneously.                                 |
| `test_gdrive_service_reuse`          | Check how many times `googleapiclient.discovery.build` is called.                  | It should be called once (or cached) instead of for every file.                                 |
| `test_resource_cleanup_on_failure`   | Interrupt the pipeline (e.g., CancelledError).                                     | Temporary files should still be cleaned up properly.                                            |

## 3. Implementation Milestones (TDD Order)

### Milestone 1: GDrive Client Optimization

- **Goal:** Fix the redundant `build()` call and prepare for async integration.
- **TDD Steps:**
    1. Write a test to verify `build()` is only called once during multiple downloads.
    2. Refactor `GDriveClient` to cache the service or initialize it once.
    3. Ensure `download_file` (singular) is robust for concurrent calls.

### Milestone 2: Async Pipeline Core

- **Goal:** Refactor `main.py` to use `asyncio.gather`.
- **TDD Steps:**
    1. Create `tests/unit/test_pipeline_parallel.py` with `test_pipeline_is_actually_parallel`.
    2. Define an async `process_file(file_info, ...)` function in `main.py` that encapsulates the
       full logic for one file.
    3. Update the `process` command to use a single `asyncio.run()` that launches all `process_file`
       tasks via `asyncio.gather()`.
    4. Use `loop.run_in_executor` for the synchronous `OCREngine.extract_text` call.

### Milestone 3: Error Handling and Robustness

- **Goal:** Ensure the parallel pipeline is resilient.
- **TDD Steps:**
    1. Write `test_pipeline_error_isolation`.
    2. Implement robust try-except blocks within `process_file` to ensure one failure doesn't stop
       the entire batch.
    3. Ensure logging remains clear even when tasks complete out of order.

### Milestone 4: Refactoring and Validation

- **Goal:** Clean up the codebase and verify everything works end-to-end.
- **Steps:**
    1. Share client instances (GDrive, OCR, Anthropic) across tasks.
    2. Clean up `asyncio.run` calls in loops by using a single entry point.
    3. Clean up old, useless functions to keep the codebase clean (e.g., `GDriveClient.download_files`).
    4. Run `tests/integration/test_cli_workflow.py`.
    5. Verify that performance is visibly improved (subjective but measurable via test duration).

## 4. Progress Tracking Checklist

### Test Cases

- [ ] `test_pipeline_is_actually_parallel` (Write: [ ], Run: [ ], Pass: [ ])
- [ ] `test_pipeline_error_isolation` (Write: [ ], Run: [ ], Pass: [ ])
- [ ] `test_async_api_concurrency` (Write: [ ], Run: [ ], Pass: [ ])
- [ ] `test_gdrive_service_reuse` (Write: [ ], Run: [ ], Pass: [ ])
- [ ] `test_resource_cleanup_on_failure` (Write: [ ], Run: [ ], Pass: [ ])

### Implementation Milestones

- [ ] **Milestone 1: GDrive Client Optimization**
    - [ ] Cache Google Drive service instance
    - [ ] Fix redundant `build()` calls in threads
- [ ] **Milestone 2: Async Pipeline Core**
    - [ ] Implement `process_file` async function
    - [ ] Refactor `main.py` to use `asyncio.gather`
    - [ ] Use `run_in_executor` for OCR
- [ ] **Milestone 3: Error Handling and Robustness**
    - [ ] Implement per-file error isolation
    - [ ] Ensure correct reporting of partial failures
- [ ] **Milestone 4: Refactoring**
    - [ ] Shared client instances across tasks
    - [ ] Clean up `asyncio.run` calls in loops
    - [ ] Clean up old, useless functions to keep the codebase clean

### Validation & Verification

- [ ] All unit tests passing
- [ ] All integration tests passing
- [ ] Manual verification with multiple files
- [ ] Performance improvement confirmed via test logs
